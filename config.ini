# filepath: /workspaces/altlas/config.ini

[Runner]
MaxAttempts = 100000               ; Max number of training attempts per session
StuckCheckWindow = 1000            ; How many recent attempts to scan for progress
StuckThreshold = 0.04              ; Min score delta over window before considered stuck
HintProbabilityOnStuck = 0.6       ; Likelihood of injecting hints when model stalls
MaxConsecutiveStuckChecks = 3      ; Max allowed stalls before intervention
BeamSearchStagnationThreshold = 400 ; Iterations before beam cooling triggers
BeamSearchCoolingOffPeriod = 75    ; Pause period after stagnation detected
ReportFrequency = 1000             ; How often to save progress report
ReportOnSuccess = true             ; Save report immediately on task success
LogFrequency = 250                 ; Log performance stats every N attempts
TopTokensToLog = 10                ; Show top N token frequencies for debug

[Generator]
MaxGenerationLength = 200          ; Max tokens to generate per attempt
EarlyStopEntropyThreshold = 0.1    ; Entropy threshold for early stopping
EarlyStopRepetitionWindow = 10     ; Window size for repetition detection
EarlyStopRepetitionThreshold = 3   ; Min repeat length to trigger early stop

[Model]
EmbeddingDim = 96                  ; Size of token embedding vector
HiddenDim = 192                    ; LSTM hidden state size (must support attention split)
NumLayers = 2                      ; LSTM depth (deeper = more expressive)

[Optimizer]
LearningRate = 0.001               ; Initial LR for training
EntropyCoefficient = 0.20          ; Weight for entropy bonus (promotes exploration)
MinEntropyCoefficient = 0.05       ; Floor value for entropy to avoid total collapse
GradientClipNorm = 2.5             ; Prevent exploding gradients
TokenImbalancePenalty = 0.3        ; Penalty for overused token dominance
RepetitionPenalty = 0.85           ; Bias against repeated outputs
InitialGrammarBoost = 2.2          ; Token logit multiplier for grammar hints
GrammarBoostDecay = 0.985          ; How quickly grammar boosts decay
BaselineEMAAlpha = 0.25            ; Exponential average for baseline reward estimation

[Replay]
ExperienceBufferSize = 150         ; How many successful attempts to remember
ReplayProbability = 0.65            ; Chance of replaying a success instead of generating

[DynamicLR]
EnableDynamicLR = true             ; Enable auto-adjusting learning rate
MinLearningRate = 0.00005          ; Smallest LR when decayed
LRPatience = 60                    ; Patience before reducing LR after plateau

[Scorer]
SuccessThreshold = 0.92            ; Score at which a solution is considered successful

[Executor]
Timeout = 5                        ; Max seconds to run generated code

[Paths]
VocabFile = memory/vocab.json
ModelStateFile = memory/model_state.pth
OptimizerStateFile = memory/optimizer_state.pth

[Logging]
FileLogLevel = DEBUG               ; Log level for file output

[ModelFlags]
UseAttention = true
UseLayerNorm = true
UseResidual = true
UsePositionalEncoding = true