# filepath: /workspaces/altlas/config.ini

[Runner]
# Maximum number of attempts the agent will make for a task
MaxAttempts = 100000

# How many attempts to wait before checking if the agent is stuck
# Increased significantly to reduce hint frequency (Default: 15)
StuckCheckWindow = 1500

# The minimum improvement in best score required within the stuck_check_window 
# to NOT be considered stuck. Higher value means less sensitive to small improvements.
# Increased to require more significant progress (Default: 0.01)
StuckThreshold = 0.05

# Probability (0.0 to 1.0) of requesting a hint when stuck condition is met
# Increased to 0.5 for more frequent hints when stuck (Default: 0.1)
HintProbabilityOnStuck = 0.5

# How many consecutive stuck checks must fail (score not improving enough) 
# before a hint is considered (subject to hint_probability_on_stuck)
# Example: 3 means the agent must fail to improve for 3 * stuck_check_window attempts
MaxConsecutiveStuckChecks = 3

# Number of consecutive beam search attempts without improvement before triggering weight reset
BeamSearchStagnationThreshold = 500

# Number of attempts to use standard generation after a weight reset before allowing beam search again
BeamSearchCoolingOffPeriod = 200

# How often (in attempts) to log runtime statistics like token frequency
# Set to 0 or negative to disable periodic logging (Default: 500)
# How often (in attempts) to generate a training report
# Set to 0 or negative to disable report generation
ReportFrequency = 1000

# Generate a final report when a task is successfully completed
ReportOnSuccess = true


LogFrequency = 500

# How many top tokens to show in periodic frequency logs (Default: 10)
TopTokensToLog = 10

[Generator]
# Max number of tokens to generate in one attempt
# This is a fallback value for tasks that don't specify max_tokens
# Set higher to accommodate more complex tasks by default
MaxGenerationLength = 200

[Model]
# Hyperparameters for the AltLAS_RNN model
EmbeddingDim = 64
HiddenDim = 128
NumLayers = 1

[Optimizer]
# Learning rate for the Adam optimizer
LearningRate = 0.001

# Initial/Maximum coefficient for entropy regularization in the loss function
# Slightly increased to encourage more exploration (Default: 0.1)
EntropyCoefficient = 0.15

# Minimum coefficient for entropy regularization (for annealing)
# Slightly increased to encourage exploration (Default: 0.01)
MinEntropyCoefficient = 0.02

# Maximum norm for gradient clipping (helps prevent exploding gradients)
# Reduced to allow slightly larger updates
GradientClipNorm = 2.0

# Alpha for Exponential Moving Average (EMA) baseline calculation
# Controls how quickly the baseline adapts to new rewards (0 < alpha <= 1)
# Increased to adapt more quickly to reward changes
BaselineEMAAlpha = 0.2

# --- Experience Replay Configuration ---
# Size of the experience replay buffer (number of experiences to store)
# Higher values allow more past experiences to be remembered
ExperienceBufferSize = 10

# Probability of replaying a successful experience (0.0 to 1.0)
# Higher values increase the frequency of experience replay
ReplayProbability = 0.3

# --- Dynamic Learning Rate Configuration ---
# Enable/disable dynamic learning rate adjustment
EnableDynamicLR = true

# Minimum learning rate the optimizer can decrease to
MinLearningRate = 0.0001

# Number of attempts with no improvement before reducing learning rate
# Only applies when score is at least 0.5 (to prevent premature reduction)
LRPatience = 50

[Executor]
Timeout = 5

[Scorer]
SuccessThreshold = 0.99

[Paths]
# Relative paths from the project root directory
VocabFile = memory/vocab.json
ModelStateFile = memory/model_state.pth
OptimizerStateFile = memory/optimizer_state.pth

[Logging]
# FileLogLevel can be DEBUG, INFO, WARNING, ERROR, CRITICAL
FileLogLevel = INFO