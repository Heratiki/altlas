# filepath: /workspaces/altlas/config.ini

[Runner]
# Maximum number of attempts the agent will make for a task
MaxAttempts = 100000

# How many attempts to wait before checking if the agent is stuck
# Increased significantly to reduce hint frequency (Default: 15)
StuckCheckWindow = 1500

# The minimum improvement in best score required within the stuck_check_window 
# to NOT be considered stuck. Higher value means less sensitive to small improvements.
# Increased to require more significant progress (Default: 0.01)
StuckThreshold = 0.05

# Probability (0.0 to 1.0) of requesting a hint when stuck condition is met
# Lower value means hints are requested less often even when stuck
# Example: 0.1 means 10% chance (Default: 1.0 - always hint when stuck)
HintProbabilityOnStuck = 0.001

# How many consecutive stuck checks must fail (score not improving enough) 
# before a hint is considered (subject to hint_probability_on_stuck)
# Example: 3 means the agent must fail to improve for 3 * stuck_check_window attempts
MaxConsecutiveStuckChecks = 3

# How often (in attempts) to log runtime statistics like token frequency
# Set to 0 or negative to disable periodic logging (Default: 500)
LogFrequency = 500

# How many top tokens to show in periodic frequency logs (Default: 10)
TopTokensToLog = 10

[Generator]
# Max number of tokens to generate in one attempt
# This is a fallback value for tasks that don't specify max_tokens
# Set higher to accommodate more complex tasks by default
MaxGenerationLength = 200

[Model]
# Hyperparameters for the AltLAS_RNN model
EmbeddingDim = 64
HiddenDim = 128
NumLayers = 1

[Optimizer]
# Learning rate for the Adam optimizer
LearningRate = 0.001

# Initial/Maximum coefficient for entropy regularization in the loss function
# Higher value encourages more exploration (Increased from 0.01)
EntropyCoefficient = 0.1

# Minimum coefficient for entropy regularization (for annealing)
MinEntropyCoefficient = 0.001

# Maximum norm for gradient clipping (helps prevent exploding gradients)
# Increased to allow larger updates early in training
GradientClipNorm = 5.0

# Alpha for Exponential Moving Average (EMA) baseline calculation
# Controls how quickly the baseline adapts to new rewards (0 < alpha <= 1)
# Increased to adapt more quickly to reward changes
BaselineEMAAlpha = 0.2

[Executor]
Timeout = 5

[Scorer]
SuccessThreshold = 0.99

[Paths]
# Relative paths from the project root directory
VocabFile = memory/vocab.json
ModelStateFile = memory/model_state.pth
OptimizerStateFile = memory/optimizer_state.pth